{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载模型和分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    # torch_dtype=\"auto\",\n",
    "    # device_map=\"auto\"\n",
    ").to(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取ner.txt，切分成句子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "article length: 13173\n",
      "sentences: 218\n",
      "true label length: 13173\n"
     ]
    }
   ],
   "source": [
    "# 读取文件内容\n",
    "# 根据空行拆分段落\n",
    "words = []\n",
    "sentences = []\n",
    "true_labels = []\n",
    "with open(\"ner.txt\", \"r\", encoding=\"utf-8\") as f:  # 打开指定的文件\n",
    "    word = []\n",
    "    sentence = \"\"\n",
    "    true_label = []\n",
    "    while line := f.readline():  # 按行读取文件\n",
    "        line = line.strip()  # 去掉行首行尾的空白字符\n",
    "        if line:  # 如果当前行非空\n",
    "            single_word, label = line.split()  # 将行中的单词和标签拆分\n",
    "            word.append(single_word)\n",
    "            sentence += single_word\n",
    "            true_label.append(label)\n",
    "        elif sentence:  # 如果遇到空行，且句子不为空\n",
    "            words.append(word)\n",
    "            sentences.append(sentence)\n",
    "            true_labels.extend(true_label)\n",
    "            word = []\n",
    "            sentence = \"\"\n",
    "            true_label = []\n",
    "    if sentence:    # 处理最后一句\n",
    "        words.append(word)\n",
    "        sentences.append(sentence)\n",
    "        true_labels.extend(true_label)\n",
    "            \n",
    "article = ''.join(sentences)\n",
    "print(f\"article length: {len(article)}\")\n",
    "print(f\"sentences: {len(sentences)}\")\n",
    "print(f\"true label length: {len(true_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义命名实体识别的提示模板\n",
    "def create_prompt(text_chunk):\n",
    "    return f\"\"\"\n",
    "            你是一个专业的古文命名实体识别专家。请仔细阅读一下古文内容，在保证理解意思的基础上，帮我做命名实体识别任务，按照如下的提取要求来执行任务：\n",
    "            \n",
    "            请按照以下格式输出结果：\n",
    "            LOC:(此处为若干个提取出的人名，之间用'、'隔开，若没有则是空的)\n",
    "            LOC:(此处为若干个提取出的地点名，之间用'、'隔开，若没有则是空的)\n",
    "            OFI:(此处为若干个提取出的地点名，之间用'、'隔开，若没有则是空的)\n",
    "            BOOK:(此处为若干个提取出的地点名，之间用'、'隔开，若没有则是空的)\n",
    "\n",
    "            一个示例如下：\n",
    "            \n",
    "            输入文本:\n",
    "            后高宗知而深歎美之。仪凤四年薨，辍朝三日，使百官以次赴宅哭之，赠开府仪同三司、并州大都督，谥曰恭。宣帝即位，授上柱国。运之为宫正也，数进谏于帝。帝不纳，反疏忌之。时运又与王轨、宇文孝伯等皆为武帝亲待。阿剌怗木儿袭职，授虎符，緫管高丽人户。\n",
    "\n",
    "            输出:\n",
    "            PER:高宗、恭、宣帝、运、王轨、宇文孝伯、武帝、阿剌怗木儿\n",
    "            LOC:并州、高丽\n",
    "            OFI:开府仪同三司、并州大都督、上柱国、宫正\n",
    "            BOOK:\n",
    "            \n",
    "            现在把以下这段话作为输入，请给出输出结果：\n",
    "            \n",
    "            {text_chunk}\n",
    " \n",
    "            \"\"\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 输入每句话，生成prompt，运行得到每句话的预测标签\n",
    "把运行成功的每句话保存到日志中，若出错下次运行可从日志中恢复"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已从日志读取 218 句话的标签.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# 日志文件名\n",
    "journal_file = \"journal.txt\"\n",
    "\n",
    "# 已完成的标签列表\n",
    "pred_labels = []\n",
    "count = 0\n",
    "\n",
    "# 检查文件是否存在，如果不存在则创建空文件\n",
    "if not os.path.exists(journal_file):\n",
    "    open(journal_file, \"w\", encoding=\"utf-8\").close()\n",
    "\n",
    "# 读取已完成的标签\n",
    "if os.path.exists(journal_file):\n",
    "    with open(journal_file, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            line_list = line.strip().split(\" \")  # 去掉换行符后按空格分隔\n",
    "            count += 1\n",
    "            pred_labels.extend(line_list)\n",
    "        \n",
    "print(f\"已从日志读取 {count} 句话的标签.\")\n",
    "\n",
    "# for i, sentence in enumerate(sentences):\n",
    "for i in range(count, len(sentences)):\n",
    "    print(f\"正在处理第 {i+1} 段...\")\n",
    "\n",
    "    # 构建每段的 prompt\n",
    "    sentence = sentences[i]\n",
    "    prompt = create_prompt(sentence)\n",
    "    \n",
    "    print(\"prompt OK\")\n",
    "    \n",
    "    # 准备输入数据\n",
    "    messages = [\n",
    "        # {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    print(\"start gen\")\n",
    "    \n",
    "    # 生成输出\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=512\n",
    "    )\n",
    "    # 处理生成的结果\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "    print(\"response OK\")\n",
    "    \n",
    "    # 解析并分类实体\n",
    "    entities = {\"PER\": [], \"LOC\": [], \"OFI\": [], \"BOOK\": []}\n",
    "\n",
    "    for line in response.split(\"\\n\"):\n",
    "        if line.startswith(\"PER:\"):\n",
    "            entities[\"PER\"] = line.replace(\"PER:\", \"\").strip().split(\"、\")\n",
    "        elif line.startswith(\"LOC:\"):\n",
    "            entities[\"LOC\"] = line.replace(\"LOC:\", \"\").strip().split(\"、\")\n",
    "        elif line.startswith(\"OFI:\"):\n",
    "            entities[\"OFI\"] = line.replace(\"OFI:\", \"\").strip().split(\"、\")\n",
    "        elif line.startswith(\"BOOK:\"):\n",
    "            entities[\"BOOK\"] = line.replace(\"BOOK:\", \"\").strip().split(\"、\")\n",
    "            \n",
    "    print(\"entities OK\")\n",
    "            \n",
    "    pred_label = ['O'] * len(sentence)\n",
    "    \n",
    "    # 定义标签类型\n",
    "    label_types = [\"PER\", \"LOC\", \"OFI\", \"BOOK\"]\n",
    "\n",
    "    # 遍历每种实体类型\n",
    "    for label_type in label_types:\n",
    "        for entity in entities[label_type]:\n",
    "            if not entity:\n",
    "                continue\n",
    "            start = sentence.find(entity)\n",
    "            if start != -1:\n",
    "                end = start + len(entity)\n",
    "                if 0 <= start < len(pred_label) and 0 < end <= len(pred_label):\n",
    "                    if len(entity) == 1:  # 单字实体\n",
    "                        pred_label[start] = f\"S-{label_type}\"\n",
    "                    else:  # 多字实体\n",
    "                        pred_label[start] = f\"B-{label_type}\"  # 开始\n",
    "                        pred_label[end - 1] = f\"E-{label_type}\"  # 结束\n",
    "                        if end - start > 2:\n",
    "                            pred_label[start + 1:end - 1] = [f\"I-{label_type}\"] * (len(entity) - 2)  # 中间\n",
    "                    \n",
    "    print(f\"sentence length: {len(sentence)}, label length: {len(pred_label)}\")\n",
    "    assert len(sentence) == len(pred_label), \"LABEL_LENGTH_ERROR\"\n",
    "    \n",
    "    pred_labels.extend(pred_label)\n",
    "    \n",
    "    print(\"label OK\") \n",
    "    \n",
    "    # 实时写入日志\n",
    "    with open(journal_file, \"a\", encoding=\"utf-8\") as file: # 'a' 模式表示追加写入\n",
    "        file.write(\" \".join(pred_label) + \"\\n\")  # 将 list 转为空格分隔的字符串并换行\n",
    "\n",
    "    print(f\"第 {i+1} 段处理完成，结果已保存到日志.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算f1分数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_marco: 0.33181592684168515\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1 = f1_score(true_labels, pred_labels, average='macro')\n",
    "print(f\"f1_marco: {f1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
